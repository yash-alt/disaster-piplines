{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yashvaishnav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yashvaishnav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///Messages.db')\n",
    "df = pd.read_sql(\"SELECT * FROM Messages\", engine)\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    messages\n",
    "       \n",
    "    Returns:\n",
    "    list of words into numbers of same meaning\n",
    "    \"\"\"\n",
    "    # Converting everything to lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # normalization word tokens and remove stop words\n",
    "    normlizer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    normlized = [normlizer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return normlized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators='warn',\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)\n",
    "\n",
    "np.random.seed(13)\n",
    "pipe.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(ArrayL, ArrayP, col_names):\n",
    "    metrics = []\n",
    "    \n",
    "    # Evaluate metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        precision = precision_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        recall = recall_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        f1 = f1_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # store metrics\n",
    "    metrics = np.array(metrics)\n",
    "    data_metrics = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return data_metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.990267   0.991863  0.995448  0.993652\n",
      "request                 0.987091   0.996161  0.928444  0.961111\n",
      "offer                   0.998566   1.000000  0.705263  0.827160\n",
      "aid_related             0.986374   0.994873  0.972497  0.983558\n",
      "medical_help            0.988576   0.996302  0.860703  0.923552\n",
      "medical_products        0.992470   1.000000  0.851815  0.919978\n",
      "search_and_rescue       0.994416   0.993590  0.814361  0.895091\n",
      "security                0.995953   0.989051  0.780980  0.872786\n",
      "military                0.995594   0.994614  0.869702  0.927973\n",
      "water                   0.994416   1.000000  0.913217  0.954640\n",
      "food                    0.995390   0.998111  0.960891  0.979147\n",
      "shelter                 0.992777   0.998748  0.919885  0.957696\n",
      "clothing                0.997951   0.992647  0.876623  0.931034\n",
      "money                   0.996568   1.000000  0.855292  0.922002\n",
      "missing_people          0.997234   1.000000  0.750000  0.857143\n",
      "refugees                0.994570   1.000000  0.832808  0.908778\n",
      "death                   0.994672   1.000000  0.884573  0.938751\n",
      "other_aid               0.977614   0.995379  0.834560  0.907903\n",
      "infrastructure_related  0.985452   0.997982  0.778127  0.874447\n",
      "transport               0.990933   0.998624  0.804878  0.891344\n",
      "buildings               0.992726   0.998830  0.858291  0.923243\n",
      "electricity             0.995492   1.000000  0.782178  0.877778\n",
      "tools                   0.998207   1.000000  0.713115  0.832536\n",
      "hospitals               0.996824   1.000000  0.721973  0.838542\n",
      "shops                   0.998412   1.000000  0.643678  0.783217\n",
      "aid_centers             0.996824   1.000000  0.733906  0.846535\n",
      "other_infrastructure    0.990216   0.996992  0.778169  0.874094\n",
      "weather_related         0.989550   0.996402  0.966036  0.980984\n",
      "floods                  0.991650   0.995924  0.903266  0.947334\n",
      "storm                   0.993289   0.998226  0.929515  0.962646\n",
      "fire                    0.998053   1.000000  0.829596  0.906863\n",
      "earthquake              0.996107   0.996546  0.961133  0.978519\n",
      "cold                    0.997234   1.000000  0.862245  0.926027\n",
      "other_weather           0.989806   0.997608  0.808923  0.893412\n",
      "direct_report           0.982019   0.997392  0.909619  0.951486\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "Y_train_pred = pipe.predict(X_train)\n",
    "col_names = list(Y.columns.values)\n",
    "\n",
    "print(eval_metrics(np.array(Y_train), Y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.802367   0.843826  0.909402  0.875388\n",
      "request                 0.883510   0.790997  0.439286  0.564868\n",
      "offer                   0.996465   0.000000  0.000000  0.000000\n",
      "aid_related             0.741048   0.722272  0.602837  0.657172\n",
      "medical_help            0.919932   0.484848  0.061657  0.109402\n",
      "medical_products        0.953435   0.673077  0.109034  0.187668\n",
      "search_and_rescue       0.976794   0.562500  0.058824  0.106509\n",
      "security                0.980483   0.000000  0.000000  0.000000\n",
      "military                0.967420   0.703704  0.085202  0.152000\n",
      "water                   0.944675   0.780000  0.187500  0.302326\n",
      "food                    0.929461   0.813239  0.475138  0.599826\n",
      "shelter                 0.931612   0.813084  0.300518  0.438840\n",
      "clothing                0.985400   0.600000  0.061856  0.112150\n",
      "money                   0.978638   0.600000  0.042553  0.079470\n",
      "missing_people          0.987245   0.000000  0.000000  0.000000\n",
      "refugees                0.964500   0.857143  0.049793  0.094118\n",
      "death                   0.959121   0.787234  0.126280  0.217647\n",
      "other_aid               0.866298   0.462687  0.035838  0.066524\n",
      "infrastructure_related  0.932688   0.250000  0.004608  0.009050\n",
      "transport               0.954511   0.571429  0.040134  0.075000\n",
      "buildings               0.951744   0.714286  0.118343  0.203046\n",
      "electricity             0.980329   0.500000  0.015625  0.030303\n",
      "tools                   0.994314   0.000000  0.000000  0.000000\n",
      "hospitals               0.990779   0.500000  0.016667  0.032258\n",
      "shops                   0.994929   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988320   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.953435   0.166667  0.003344  0.006557\n",
      "weather_related         0.853542   0.831976  0.607568  0.702281\n",
      "floods                  0.946058   0.863454  0.404135  0.550576\n",
      "storm                   0.926233   0.785992  0.322169  0.457014\n",
      "fire                    0.991087   1.000000  0.016949  0.033333\n",
      "earthquake              0.961734   0.897839  0.698777  0.785899\n",
      "cold                    0.978792   0.500000  0.043478  0.080000\n",
      "other_weather           0.945597   0.320000  0.023188  0.043243\n",
      "direct_report           0.837713   0.697479  0.321456  0.440085\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "Y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "eval_metrics0 = eval_metrics(np.array(Y_test), Y_test_pred, col_names)\n",
    "print(eval_metrics0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   0.764792\n",
       "request                   0.171892\n",
       "offer                     0.004534\n",
       "aid_related               0.417243\n",
       "medical_help              0.080068\n",
       "medical_products          0.050446\n",
       "search_and_rescue         0.027816\n",
       "security                  0.018096\n",
       "military                  0.033041\n",
       "water                     0.064239\n",
       "food                      0.112302\n",
       "shelter                   0.088904\n",
       "clothing                  0.015560\n",
       "money                     0.023206\n",
       "missing_people            0.011449\n",
       "refugees                  0.033618\n",
       "death                     0.045874\n",
       "other_aid                 0.132396\n",
       "infrastructure_related    0.065506\n",
       "transport                 0.046143\n",
       "buildings                 0.051214\n",
       "electricity               0.020440\n",
       "tools                     0.006109\n",
       "hospitals                 0.010873\n",
       "shops                     0.004610\n",
       "aid_centers               0.011872\n",
       "other_infrastructure      0.044222\n",
       "weather_related           0.280352\n",
       "floods                    0.082795\n",
       "storm                     0.093860\n",
       "fire                      0.010834\n",
       "earthquake                0.094321\n",
       "cold                      0.020363\n",
       "other_weather             0.052866\n",
       "direct_report             0.194982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculation the proportion of each column that have label == 1\n",
    "Y.sum()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metric for use in grid search scoring object\n",
    "def perform_metric(Y_T, Y_P):\n",
    "    \"\"\"Median F1 score for all classifiers\n",
    "    \n",
    "    inputs:\n",
    "    Y_T: array. Array containing ArrayL labels.\n",
    "    Y_P: array. Array containing ArrayP labels.\n",
    "        \n",
    "    Routputs:\n",
    "    Median F1 score for all  classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(Y_P)[1]):\n",
    "        f1 = f1_score(np.array(Y_T)[:, i], Y_P[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.127, total=  35.3s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.110, total=  35.0s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.122, total=  35.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.150, total=  30.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.175, total=  30.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.191, total=  30.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  3.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.136, total=  35.5s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  3.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.116, total=  35.3s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  4.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.115, total=  36.0s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  5.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.153, total=  29.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.160, total=  29.9s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.160, total=  30.5s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.116, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.138, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.124, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.231, total=  55.2s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.222, total=  55.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.188, total=  56.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.130, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.136, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.097, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.216, total=  53.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.190, total=  56.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.130, total=  58.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.134, total=  34.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.197, total=  32.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.140, total=  32.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.234, total=  28.8s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.218, total=  28.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.186, total=  28.5s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.164, total=  30.9s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.178, total=  31.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.133, total=  31.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.202, total=  28.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.203, total=  28.2s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.163, total=  27.8s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.113, total=  56.0s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.118, total=  56.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.100, total=  56.7s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.204, total=  49.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.195, total=  48.9s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.130, total=  49.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.165, total=  55.8s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.132, total=  55.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.141, total=  55.9s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.216, total=  47.5s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.194, total=  48.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.180, total=  47.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.196, total=  31.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.149, total=  30.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.195, total=  30.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.228, total=  27.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.179, total=  27.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.203, total=  27.7s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.175, total=  29.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.098, total=  29.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.122, total=  29.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.222, total=  26.7s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.233, total=  26.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.199, total=  26.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.121, total=  51.8s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.089, total=  52.7s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.097, total=  52.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.215, total=  47.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.194, total=  46.7s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.193, total=  47.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.127, total=  50.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.142, total=  50.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.096, total=  51.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.192, total=  44.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.203, total=  44.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.144, total=  44.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 50.8min finished\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "\n",
    "parameters = {'vect__min_df': [1, 5],\n",
    "              'tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__n_estimators':[10, 25], \n",
    "              'clf__estimator__min_samples_split':[2, 5, 10]}\n",
    "\n",
    "scorer = make_scorer(perform_metric)\n",
    "cv = GridSearchCV(pipe, param_grid = parameters, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(74)\n",
    "T_model = cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([27.61548702, 23.58777372, 27.96730312, 22.93114368, 55.39306124,\n",
       "        45.26894291, 55.79999304, 45.4797283 , 24.79149199, 21.66640814,\n",
       "        23.4649663 , 20.98384778, 44.58539677, 39.08501808, 44.1311926 ,\n",
       "        37.4480656 , 22.54718995, 20.6393942 , 21.54416498, 19.66050458,\n",
       "        40.25510192, 36.82695341, 39.07299391, 34.44030333]),\n",
       " 'std_fit_time': array([0.26013814, 0.12902099, 0.37241319, 0.29752232, 0.22826761,\n",
       "        0.72105078, 0.73842418, 1.77857054, 1.05544227, 0.10989888,\n",
       "        0.10364863, 0.12537212, 0.23782144, 0.21330336, 0.18688452,\n",
       "        0.19222951, 0.07139557, 0.06383181, 0.12935219, 0.06739355,\n",
       "        0.26639054, 0.20870047, 0.46749838, 0.16862085]),\n",
       " 'mean_score_time': array([ 7.68746837,  7.04871837,  7.63337501,  7.09826231, 11.96501939,\n",
       "        10.60245617, 11.73090164, 10.59458907,  8.12793628,  6.99770284,\n",
       "         7.61334443,  7.01815502, 11.76693193, 10.09308147, 11.66546822,\n",
       "        10.2842923 ,  7.97118203,  6.99848962,  7.58603398,  6.94420624,\n",
       "        11.99134103, 10.10125422, 11.57101862,  9.92147605]),\n",
       " 'std_score_time': array([0.04903825, 0.11726777, 0.05585016, 0.14065547, 0.25941239,\n",
       "        0.11280302, 0.0864615 , 0.30468509, 0.25613064, 0.03397002,\n",
       "        0.06041481, 0.06835696, 0.0467189 , 0.02380585, 0.05612853,\n",
       "        0.35266203, 0.29193456, 0.04463384, 0.06968824, 0.03601052,\n",
       "        0.09619369, 0.02298337, 0.04540718, 0.05754629]),\n",
       " 'param_clf__estimator__min_samples_split': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data=[10, 10, 10, 10, 25, 25, 25, 25, 10, 10, 10, 10, 25, 25,\n",
       "                    25, 25, 10, 10, 10, 10, 25, 25, 25, 25],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[True, True, False, False, True, True, False, False,\n",
       "                    True, True, False, False, True, True, False, False,\n",
       "                    True, True, False, False, True, True, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__min_df': masked_array(data=[1, 5, 1, 5, 1, 5, 1, 5, 1, 5, 1, 5, 1, 5, 1, 5, 1, 5,\n",
       "                    1, 5, 1, 5, 1, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5}],\n",
       " 'split0_test_score': array([0.12711864, 0.15019763, 0.13623978, 0.15339233, 0.11634349,\n",
       "        0.23054755, 0.13043478, 0.21621622, 0.13385827, 0.23404255,\n",
       "        0.16413374, 0.2020202 , 0.11320755, 0.20408163, 0.16470588,\n",
       "        0.21637427, 0.1958042 , 0.22773723, 0.175     , 0.22222222,\n",
       "        0.12087912, 0.21538462, 0.12711864, 0.19230769]),\n",
       " 'split1_test_score': array([0.11009174, 0.17486339, 0.11555556, 0.15976331, 0.13793103,\n",
       "        0.22222222, 0.13559322, 0.18965517, 0.19672131, 0.21848739,\n",
       "        0.1779661 , 0.2032967 , 0.11764706, 0.1954023 , 0.13215859,\n",
       "        0.19433198, 0.14880952, 0.17927171, 0.09798271, 0.23312883,\n",
       "        0.08928571, 0.19428571, 0.14218009, 0.20264317]),\n",
       " 'split2_test_score': array([0.12195122, 0.19117647, 0.1147541 , 0.15957447, 0.12403101,\n",
       "        0.18823529, 0.09677419, 0.13023256, 0.13953488, 0.18579235,\n",
       "        0.13297872, 0.16296296, 0.1       , 0.13023256, 0.14084507,\n",
       "        0.18045113, 0.19537275, 0.20289855, 0.12195122, 0.19852941,\n",
       "        0.09716599, 0.19259259, 0.096     , 0.14367816]),\n",
       " 'mean_test_score': array([0.11972054, 0.17207916, 0.12218315, 0.1575767 , 0.12610184,\n",
       "        0.21366836, 0.12093407, 0.17870132, 0.15670482, 0.2127741 ,\n",
       "        0.15835952, 0.18942662, 0.11028487, 0.17657216, 0.14590318,\n",
       "        0.19705246, 0.17999549, 0.2033025 , 0.13164464, 0.21796016,\n",
       "        0.10244361, 0.20075431, 0.12176625, 0.17954301]),\n",
       " 'std_test_score': array([0.00712792, 0.01684499, 0.00994493, 0.0029598 , 0.0089339 ,\n",
       "        0.01830224, 0.01721292, 0.03594707, 0.02839067, 0.02010807,\n",
       "        0.0188144 , 0.01871989, 0.00749494, 0.03295807, 0.01376033,\n",
       "        0.01479119, 0.02205251, 0.01978803, 0.03218061, 0.01444308,\n",
       "        0.01342698, 0.01036826, 0.01922908, 0.0257089 ]),\n",
       " 'rank_test_score': array([22, 12, 19, 14, 18,  2, 21, 10, 15,  3, 13,  7, 23, 11, 16,  6,  8,\n",
       "         4, 17,  1, 24,  5, 20,  9], dtype=int32)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search results\n",
    "T_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21796015611425212"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top mean score\n",
    "np.max(T_model.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 10,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__min_df': 5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for top mean score\n",
    "T_model.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.809590   0.838909  0.928931  0.881628\n",
      "request                 0.891809   0.796296  0.499107  0.613611\n",
      "offer                   0.996465   0.000000  0.000000  0.000000\n",
      "aid_related             0.757338   0.710084  0.693916  0.701907\n",
      "medical_help            0.920547   0.506579  0.148362  0.229508\n",
      "medical_products        0.954972   0.659091  0.180685  0.283619\n",
      "search_and_rescue       0.977870   0.695652  0.104575  0.181818\n",
      "security                0.980175   0.000000  0.000000  0.000000\n",
      "military                0.968034   0.741935  0.103139  0.181102\n",
      "water                   0.959428   0.811475  0.475962  0.600000\n",
      "food                    0.945597   0.794586  0.689227  0.738166\n",
      "shelter                 0.939911   0.767045  0.466321  0.580021\n",
      "clothing                0.985861   0.608696  0.144330  0.233333\n",
      "money                   0.979099   0.608696  0.099291  0.170732\n",
      "missing_people          0.987398   0.500000  0.012195  0.023810\n",
      "refugees                0.965422   0.710526  0.112033  0.193548\n",
      "death                   0.962963   0.906250  0.197952  0.324930\n",
      "other_aid               0.868603   0.535714  0.086705  0.149254\n",
      "infrastructure_related  0.931766   0.142857  0.004608  0.008929\n",
      "transport               0.955433   0.578947  0.110368  0.185393\n",
      "buildings               0.952052   0.732143  0.121302  0.208122\n",
      "electricity             0.980483   0.555556  0.039062  0.072993\n",
      "tools                   0.994314   0.000000  0.000000  0.000000\n",
      "hospitals               0.990779   0.000000  0.000000  0.000000\n",
      "shops                   0.994929   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988320   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.953127   0.125000  0.003344  0.006515\n",
      "weather_related         0.871216   0.812732  0.710811  0.758362\n",
      "floods                  0.950976   0.865979  0.473684  0.612394\n",
      "storm                   0.942677   0.760246  0.591707  0.665471\n",
      "fire                    0.991087   1.000000  0.016949  0.033333\n",
      "earthquake              0.970032   0.892308  0.798165  0.842615\n",
      "cold                    0.980636   0.700000  0.152174  0.250000\n",
      "other_weather           0.945751   0.420000  0.060870  0.106329\n",
      "direct_report           0.845397   0.709251  0.374129  0.489858\n"
     ]
    }
   ],
   "source": [
    "# evaluating metrics for test set\n",
    "tuned_pred_test = T_model.predict(X_test)\n",
    "\n",
    "eval_metrics1 = eval_metrics(np.array(Y_test), tuned_pred_test, col_names)\n",
    "print(eval_metrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.941434</td>\n",
       "      <td>0.545535</td>\n",
       "      <td>0.176633</td>\n",
       "      <td>0.228930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.311056</td>\n",
       "      <td>0.238942</td>\n",
       "      <td>0.263613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.741048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.930536</td>\n",
       "      <td>0.391343</td>\n",
       "      <td>0.016146</td>\n",
       "      <td>0.031281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.954511</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.106509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.980406</td>\n",
       "      <td>0.789115</td>\n",
       "      <td>0.310987</td>\n",
       "      <td>0.439462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.996465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909402</td>\n",
       "      <td>0.875388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.941434   0.545535   0.176633   0.228930\n",
       "std     0.058766   0.311056   0.238942   0.263613\n",
       "min     0.741048   0.000000   0.000000   0.000000\n",
       "25%     0.930536   0.391343   0.016146   0.031281\n",
       "50%     0.954511   0.600000   0.058824   0.106509\n",
       "75%     0.980406   0.789115   0.310987   0.439462\n",
       "max     0.996465   1.000000   0.909402   0.875388"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of 1st model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.945430</td>\n",
       "      <td>0.556759</td>\n",
       "      <td>0.239997</td>\n",
       "      <td>0.295066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.055305</td>\n",
       "      <td>0.314854</td>\n",
       "      <td>0.275218</td>\n",
       "      <td>0.286459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.757338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.941294</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.959428</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.112033</td>\n",
       "      <td>0.193548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.980559</td>\n",
       "      <td>0.780816</td>\n",
       "      <td>0.470003</td>\n",
       "      <td>0.590011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.996465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928931</td>\n",
       "      <td>0.881628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.945430   0.556759   0.239997   0.295066\n",
       "std     0.055305   0.314854   0.275218   0.286459\n",
       "min     0.757338   0.000000   0.000000   0.000000\n",
       "25%     0.941294   0.460000   0.014572   0.028571\n",
       "50%     0.959428   0.695652   0.112033   0.193548\n",
       "75%     0.980559   0.780816   0.470003   0.590011\n",
       "max     0.996465   1.000000   0.928931   0.881628"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of tuned model\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.5min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  9.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 14.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 16.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.3min\n",
      "[CV] clf__estimator__C=1, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 19.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=1, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 21.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__C=10, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.8min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.8min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.8min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.3min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=10, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=10, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.3min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 3.2min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 3.2min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=1, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 3.2min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=2, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.4min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.3min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.3min\n",
      "[CV] clf__estimator__C=100, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__C=100, clf__estimator__degree=3, clf__estimator__kernel=poly, tfidf__use_idf=True, vect__min_df=5, score=0.000, total= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed: 67.7min finished\n"
     ]
    }
   ],
   "source": [
    "# using SVM inste ad \n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(SVC()))\n",
    "])\n",
    "\n",
    "parameters2 = {'vect__min_df': [5],\n",
    "              'tfidf__use_idf':[True],\n",
    "              'clf__estimator__kernel': ['poly'], \n",
    "              'clf__estimator__degree': [1, 2, 3],\n",
    "              'clf__estimator__C':[1, 10, 100]}\n",
    "\n",
    "cv2 = GridSearchCV(pipe2, param_grid = parameters2, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(77)\n",
    "T_model2 = cv2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([100.79459023,  97.05863945,  95.92652734, 114.96100084,\n",
       "         96.82942669,  95.72683231, 135.52718973,  97.22322369,\n",
       "         95.14110033]),\n",
       " 'std_fit_time': array([0.84848527, 0.52813645, 0.49461939, 0.28684776, 0.41033756,\n",
       "        0.20308257, 1.29025647, 0.18485533, 0.42341499]),\n",
       " 'mean_score_time': array([45.68554211, 45.4884096 , 45.33674184, 50.54041243, 45.42993752,\n",
       "        45.20480498, 57.34199667, 45.46698268, 45.15871962]),\n",
       " 'std_score_time': array([0.14818545, 0.10372381, 0.07531002, 0.04815436, 0.09377946,\n",
       "        0.10819078, 0.13214101, 0.13191711, 0.11317324]),\n",
       " 'param_clf__estimator__C': masked_array(data=[1, 1, 1, 10, 10, 10, 100, 100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__degree': masked_array(data=[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__kernel': masked_array(data=['poly', 'poly', 'poly', 'poly', 'poly', 'poly', 'poly',\n",
       "                    'poly', 'poly'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[True, True, True, True, True, True, True, True, True],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__min_df': masked_array(data=[5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__estimator__C': 1,\n",
       "   'clf__estimator__degree': 1,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 1,\n",
       "   'clf__estimator__degree': 2,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 1,\n",
       "   'clf__estimator__degree': 3,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 10,\n",
       "   'clf__estimator__degree': 1,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 10,\n",
       "   'clf__estimator__degree': 2,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 10,\n",
       "   'clf__estimator__degree': 3,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 100,\n",
       "   'clf__estimator__degree': 1,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 100,\n",
       "   'clf__estimator__degree': 2,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__C': 100,\n",
       "   'clf__estimator__degree': 3,\n",
       "   'clf__estimator__kernel': 'poly',\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5}],\n",
       " 'split0_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'split1_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'split2_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'mean_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'rank_test_score': array([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search results\n",
    "T_model2.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "median always zero therefore lets evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision  Recall        F1\n",
      "related                 0.763332   0.763332     1.0  0.865784\n",
      "request                 0.827878   0.000000     0.0  0.000000\n",
      "offer                   0.996465   0.000000     0.0  0.000000\n",
      "aid_related             0.588290   0.000000     0.0  0.000000\n",
      "medical_help            0.920240   0.000000     0.0  0.000000\n",
      "medical_products        0.950669   0.000000     0.0  0.000000\n",
      "search_and_rescue       0.976487   0.000000     0.0  0.000000\n",
      "security                0.980944   0.000000     0.0  0.000000\n",
      "military                0.965729   0.000000     0.0  0.000000\n",
      "water                   0.936069   0.000000     0.0  0.000000\n",
      "food                    0.888735   0.000000     0.0  0.000000\n",
      "shelter                 0.911019   0.000000     0.0  0.000000\n",
      "clothing                0.985093   0.000000     0.0  0.000000\n",
      "money                   0.978331   0.000000     0.0  0.000000\n",
      "missing_people          0.987398   0.000000     0.0  0.000000\n",
      "refugees                0.962963   0.000000     0.0  0.000000\n",
      "death                   0.954972   0.000000     0.0  0.000000\n",
      "other_aid               0.867066   0.000000     0.0  0.000000\n",
      "infrastructure_related  0.933303   0.000000     0.0  0.000000\n",
      "transport               0.954049   0.000000     0.0  0.000000\n",
      "buildings               0.948056   0.000000     0.0  0.000000\n",
      "electricity             0.980329   0.000000     0.0  0.000000\n",
      "tools                   0.994314   0.000000     0.0  0.000000\n",
      "hospitals               0.990779   0.000000     0.0  0.000000\n",
      "shops                   0.994929   0.000000     0.0  0.000000\n",
      "aid_centers             0.988320   0.000000     0.0  0.000000\n",
      "other_infrastructure    0.954049   0.000000     0.0  0.000000\n",
      "weather_related         0.715691   0.000000     0.0  0.000000\n",
      "floods                  0.918242   0.000000     0.0  0.000000\n",
      "storm                   0.903642   0.000000     0.0  0.000000\n",
      "fire                    0.990933   0.000000     0.0  0.000000\n",
      "earthquake              0.899493   0.000000     0.0  0.000000\n",
      "cold                    0.978792   0.000000     0.0  0.000000\n",
      "other_weather           0.946980   0.000000     0.0  0.000000\n",
      "direct_report           0.801598   0.000000     0.0  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate  evaluation metrics for test set\n",
    "tuned_pred_test2 = T_model2.predict(X_test)\n",
    "#todo calculate the eval metrics \n",
    "eval_metrics2 = eval_metrics(np.array(Y_test), tuned_pred_test2, col_names)\n",
    "\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM doesnt work ,we will be using RANDOM FOREST *FTW*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle best model\n",
    "pickle.dump(T_model, open('disaster_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
